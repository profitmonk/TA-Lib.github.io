{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtuStkWyVtHpwI5Z3b9TeJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/profitmonk/TA-Lib.github.io/blob/main/Prod_10032024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "2IDn05Q2Phko",
        "outputId": "57ac3fc3-601f-464a-b426-f0e345ed0db9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying block matrix multiplication...\n",
            "hey 5 5\n",
            "Block Matrix Multiplication is correct!\n",
            "\n",
            "Simulating ResNet-18...\n",
            "hey 224 7\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Matrix dimensions do not match for multiplication.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-26ef10f0232b>\u001b[0m in \u001b[0;36m<cell line: 209>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;31m# Initialize and simulate ResNet-18\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0mresnet18_simulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet18Simulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet18_simulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate_resnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_kernels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ResNet-18 Output (after Fully Connected Layer):\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-26ef10f0232b>\u001b[0m in \u001b[0;36msimulate_resnet\u001b[0;34m(self, input_tensor, conv_kernels)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \"\"\"\n\u001b[1;32m    163\u001b[0m         \u001b[0;31m# Initial Conv + MaxPooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooling_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-26ef10f0232b>\u001b[0m in \u001b[0;36minitial_conv\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# Initial Conv Layer (7x7 kernel, 64 filters, stride 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mconv_kernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet_simulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_kernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimulate_resnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_kernels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-26ef10f0232b>\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(self, input_tensor, kernel)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# Simulating convolution as a matrix multiplication for simplicity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_multiplier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-26ef10f0232b>\u001b[0m in \u001b[0;36mmultiply_blocks\u001b[0;34m(self, matrix_a, matrix_b)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moriginal_k2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hey\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moriginal_k1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moriginal_k2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0moriginal_k1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moriginal_k2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Matrix dimensions do not match for multiplication.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Calculate the padded dimensions to make them multiples of the block size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Matrix dimensions do not match for multiplication."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Block Matrix Multiplier\n",
        "class BlockMatrixMultiplier:\n",
        "    def __init__(self, block_size, input_precision='INT8', accumulator_precision='INT32'):\n",
        "        self.block_size = block_size  # Block size NxN\n",
        "        self.input_precision = input_precision\n",
        "        self.accumulator_precision = accumulator_precision\n",
        "\n",
        "    def pad_matrix(self, matrix, target_shape):\n",
        "        \"\"\"\n",
        "        Pads the matrix to the target shape.\n",
        "        matrix: Input matrix to be padded\n",
        "        target_shape: (rows, cols) - shape to pad the matrix to\n",
        "        \"\"\"\n",
        "        original_shape = matrix.shape\n",
        "        padded_matrix = np.zeros(target_shape, dtype=matrix.dtype)\n",
        "        padded_matrix[:original_shape[0], :original_shape[1]] = matrix\n",
        "        return padded_matrix\n",
        "\n",
        "    def multiply_blocks(self, matrix_a, matrix_b):\n",
        "        \"\"\"\n",
        "        Simulates block matrix multiplication for quantized inputs with padding.\n",
        "        matrix_a: (MxK), matrix_b: (KxN), Block Size: (NxN)\n",
        "        \"\"\"\n",
        "        # Determine original matrix dimensions\n",
        "        original_m, original_k1 = matrix_a.shape\n",
        "        original_k2, original_n = matrix_b.shape\n",
        "        print(\"hey\",original_k1,original_k2)\n",
        "        assert original_k1 == original_k2, \"Matrix dimensions do not match for multiplication.\"\n",
        "\n",
        "        # Calculate the padded dimensions to make them multiples of the block size\n",
        "        padded_m = ((original_m + self.block_size - 1) // self.block_size) * self.block_size\n",
        "        padded_k = ((original_k1 + self.block_size - 1) // self.block_size) * self.block_size\n",
        "        padded_n = ((original_n + self.block_size - 1) // self.block_size) * self.block_size\n",
        "\n",
        "        # Pad the matrices\n",
        "        matrix_a_padded = self.pad_matrix(matrix_a, (padded_m, padded_k))\n",
        "        matrix_b_padded = self.pad_matrix(matrix_b, (padded_k, padded_n))\n",
        "\n",
        "        # Initialize the padded result matrix\n",
        "        #result_matrix_padded = np.zeros((padded_m, padded_n), dtype=self.accumulator_precision)\n",
        "        result_matrix_padded = np.zeros((padded_m, padded_n), dtype=np.int32 if self.accumulator_precision == 'INT32' else np.float32) # Changed 'INT32' to np.int32\n",
        "        # Convert matrices to the specified precision\n",
        "        matrix_a_padded = matrix_a_padded.astype(np.int8 if self.input_precision == 'INT8' else np.float32)\n",
        "        matrix_b_padded = matrix_b_padded.astype(np.int8 if self.input_precision == 'INT8' else np.float32)\n",
        "\n",
        "        # Block matrix multiplication\n",
        "        for i in range(0, padded_m, self.block_size):\n",
        "            for j in range(0, padded_n, self.block_size):\n",
        "                for k in range(0, padded_k, self.block_size):\n",
        "                    # Extract blocks of size NxN (or smaller if near boundary)\n",
        "                    a_block = matrix_a_padded[i:i+self.block_size, k:k+self.block_size]\n",
        "                    b_block = matrix_b_padded[k:k+self.block_size, j:j+self.block_size]\n",
        "\n",
        "                    # Perform block multiplication\n",
        "                    partial_result = np.dot(a_block, b_block)\n",
        "\n",
        "                    # Accumulate the result in the final matrix\n",
        "                    result_matrix_padded[i:i+self.block_size, j:j+self.block_size] += partial_result.astype(np.int32 if self.accumulator_precision == 'INT32' else np.float32)\n",
        "\n",
        "        # Trim the result matrix back to the original size\n",
        "        result_matrix = result_matrix_padded[:original_m, :original_n]\n",
        "\n",
        "        return result_matrix\n",
        "\n",
        "\n",
        "# Element-Wise Operations\n",
        "class ElementWiseOperations:\n",
        "    def __init__(self, input_precision='INT8'):\n",
        "        self.input_precision = input_precision\n",
        "\n",
        "    def add(self, tensor_a, tensor_b):\n",
        "        tensor_a = tensor_a.astype(np.int8)\n",
        "        tensor_b = tensor_b.astype(np.int8)\n",
        "        return np.add(tensor_a, tensor_b).astype(np.int32)\n",
        "\n",
        "\n",
        "# Activation Functions\n",
        "class ActivationFunctions:\n",
        "    def relu(self, tensor):\n",
        "        return np.maximum(tensor, 0)\n",
        "\n",
        "\n",
        "# Pooling Operations\n",
        "class PoolingOperations:\n",
        "    def max_pooling(self, tensor, kernel_size, stride):\n",
        "        output_shape = (\n",
        "            (tensor.shape[0] - kernel_size) // stride + 1,\n",
        "            (tensor.shape[1] - kernel_size) // stride + 1\n",
        "        )\n",
        "        pooled_tensor = np.zeros(output_shape)\n",
        "        for i in range(0, tensor.shape[0] - kernel_size + 1, stride):\n",
        "            for j in range(0, tensor.shape[1] - kernel_size + 1, stride):\n",
        "                pooled_tensor[i // stride, j // stride] = np.max(\n",
        "                    tensor[i:i + kernel_size, j:j + kernel_size])\n",
        "        return pooled_tensor\n",
        "\n",
        "\n",
        "# ResNet Block Simulator\n",
        "class ResNetBlockSimulator:\n",
        "    def __init__(self, block_size=4):\n",
        "        self.block_size = block_size\n",
        "        self.matrix_multiplier = BlockMatrixMultiplier(block_size)\n",
        "        self.element_ops = ElementWiseOperations(input_precision='INT8')\n",
        "        self.activation_funcs = ActivationFunctions()\n",
        "\n",
        "    def conv2d(self, input_tensor, kernel):\n",
        "        \"\"\"\n",
        "        Simulate a 2D convolution using matrix multiplication.\n",
        "        input_tensor: (m x n) input matrix (for simplicity)\n",
        "        kernel: (k x k) convolution kernel matrix\n",
        "        \"\"\"\n",
        "        # Simulating convolution as a matrix multiplication for simplicity\n",
        "        output = self.matrix_multiplier.multiply_blocks(input_tensor, kernel)\n",
        "        return output\n",
        "\n",
        "    def resnet_block(self, input_tensor, conv_kernel1, conv_kernel2, downsample=False):\n",
        "        \"\"\"\n",
        "        Simulates a simplified ResNet block.\n",
        "        input_tensor: Input feature map\n",
        "        conv_kernel1: First convolution kernel\n",
        "        conv_kernel2: Second convolution kernel\n",
        "        downsample: If True, simulate downsampling with stride.\n",
        "        \"\"\"\n",
        "        # First convolution followed by ReLU activation\n",
        "        conv1_output = self.conv2d(input_tensor, conv_kernel1)\n",
        "        relu1_output = self.activation_funcs.relu(conv1_output)\n",
        "\n",
        "        # Second convolution\n",
        "        conv2_output = self.conv2d(relu1_output, conv_kernel2)\n",
        "\n",
        "        # Residual connection (element-wise addition of input_tensor and conv2_output)\n",
        "        if downsample:\n",
        "            input_tensor = self.conv2d(input_tensor, np.eye(conv_kernel1.shape[0]))  # Simulate downsampling\n",
        "\n",
        "        residual_output = self.element_ops.add(input_tensor, conv2_output)\n",
        "\n",
        "        # ReLU activation on the residual output\n",
        "        final_output = self.activation_funcs.relu(residual_output)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "\n",
        "# Full ResNet-18 Implementation\n",
        "class ResNet18Simulator:\n",
        "    def __init__(self, block_size=4):\n",
        "        self.block_size = block_size\n",
        "        self.resnet_simulator = ResNetBlockSimulator(block_size)\n",
        "        self.pooling_ops = PoolingOperations()\n",
        "\n",
        "    def initial_conv(self, input_tensor):\n",
        "        # Initial Conv Layer (7x7 kernel, 64 filters, stride 2)\n",
        "        conv_kernel = np.random.randint(0, 10, size=(7, 7), dtype=np.int8)\n",
        "        return self.resnet_simulator.conv2d(input_tensor, conv_kernel)\n",
        "\n",
        "    def simulate_resnet(self, input_tensor, conv_kernels):\n",
        "        \"\"\"\n",
        "        Simulate the full ResNet-18 architecture.\n",
        "        input_tensor: Input feature map\n",
        "        conv_kernels: List of convolution kernels for the blocks\n",
        "        \"\"\"\n",
        "        # Initial Conv + MaxPooling\n",
        "        input_tensor = self.initial_conv(input_tensor)\n",
        "        input_tensor = self.pooling_ops.max_pooling(input_tensor, kernel_size=3, stride=2)\n",
        "\n",
        "        # Four stages of residual blocks\n",
        "        for i in range(0, len(conv_kernels), 4):\n",
        "            # Each stage has two residual blocks\n",
        "            conv_kernel1 = conv_kernels[i]\n",
        "            conv_kernel2 = conv_kernels[i+1]\n",
        "            input_tensor = self.resnet_simulator.resnet_block(input_tensor, conv_kernel1, conv_kernel2, downsample=(i > 0))\n",
        "\n",
        "            conv_kernel3 = conv_kernels[i+2]\n",
        "            conv_kernel4 = conv_kernels[i+3]\n",
        "            input_tensor = self.resnet_simulator.resnet_block(input_tensor, conv_kernel3, conv_kernel4)\n",
        "\n",
        "        # Global average pooling (for simplicity, we will simulate a pooling operation)\n",
        "        input_tensor = self.pooling_ops.max_pooling(input_tensor, kernel_size=input_tensor.shape[0], stride=1)\n",
        "\n",
        "        # Fully connected layer (for classification)\n",
        "        fc_weights = np.random.randint(0, 10, size=(input_tensor.size, 1000), dtype=np.int8)\n",
        "        output_tensor = np.dot(input_tensor.flatten(), fc_weights)\n",
        "\n",
        "        return output_tensor\n",
        "\n",
        "\n",
        "# Verification for Block Matrix Multiplication\n",
        "def verify_block_matrix_multiplication(matrix_a, matrix_b, block_size):\n",
        "    # Initialize the block matrix multiplier\n",
        "    block_multiplier = BlockMatrixMultiplier(block_size, input_precision='INT8', accumulator_precision='INT32')\n",
        "\n",
        "    # Perform block matrix multiplication\n",
        "    block_result = block_multiplier.multiply_blocks(matrix_a, matrix_b)\n",
        "\n",
        "    # Perform standard matrix multiplication using numpy for verification\n",
        "    standard_result = np.dot(matrix_a.astype(np.int32), matrix_b.astype(np.int32))\n",
        "\n",
        "    # Verify correctness by comparing the results\n",
        "    if np.array_equal(block_result, standard_result):\n",
        "        print(\"Block Matrix Multiplication is correct!\")\n",
        "    else:\n",
        "        print(\"Block Matrix Multiplication is incorrect!\")\n",
        "        print(\"Block Matrix Result:\\n\", block_result)\n",
        "        print(\"Standard Matrix Result:\\n\", standard_result)\n",
        "\n",
        "\n",
        "# Example usage of ResNet18Simulator and matrix multiplication checker\n",
        "if __name__ == \"__main__\":\n",
        "    # Test case for block matrix multiplication\n",
        "    matrix_a = np.random.randint(0, 2, size=(7, 5), dtype=np.int8)  # Not a multiple of block size\n",
        "    matrix_b = np.random.randint(0, 2, size=(5, 6), dtype=np.int8)  # Not a multiple of block size\n",
        "    block_size = 4\n",
        "\n",
        "    print(\"Verifying block matrix multiplication...\")\n",
        "    verify_block_matrix_multiplication(matrix_a, matrix_b, block_size)\n",
        "\n",
        "    # Test case for ResNet-18 simulation\n",
        "    print(\"\\nSimulating ResNet-18...\")\n",
        "    input_tensor = np.random.randint(0, 10, size=(224, 224), dtype=np.int8)\n",
        "\n",
        "    # Convolution kernels for ResNet-18 (8 blocks = 16 convolutions)\n",
        "    conv_kernels = [np.random.randint(0, 10, size=(3, 3), dtype=np.int8) for _ in range(16)]\n",
        "\n",
        "    # Initialize and simulate ResNet-18\n",
        "    resnet18_simulator = ResNet18Simulator(block_size=4)\n",
        "    output_tensor = resnet18_simulator.simulate_resnet(input_tensor, conv_kernels)\n",
        "    print(\"ResNet-18 Output (after Fully Connected Layer):\\n\", output_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bgPHSFJ-aF1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EuvVLg1MZ2Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_m, original_k1 = matrix_a.shape\n",
        "original_k2, original_n = matrix_b.shape"
      ],
      "metadata": {
        "id": "tL8V7-HoY2fg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFw8rb0oVtrb",
        "outputId": "80bb1862-5005-4ff4-e51e-c4ac3828cf7b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8, 7, 1, 1, 3],\n",
              "       [3, 4, 4, 9, 9],\n",
              "       [2, 5, 5, 2, 8],\n",
              "       [1, 8, 3, 5, 8],\n",
              "       [5, 4, 9, 9, 6],\n",
              "       [8, 8, 1, 4, 7],\n",
              "       [6, 0, 0, 8, 7]], dtype=int8)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn5CBbaDYlNo",
        "outputId": "6f7e767a-1eeb-43ec-8c4d-9b72e2e87410"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5, 6, 2, 1, 2, 7],\n",
              "       [1, 1, 7, 1, 9, 3],\n",
              "       [6, 9, 5, 3, 1, 3],\n",
              "       [0, 5, 5, 1, 0, 3],\n",
              "       [1, 6, 9, 8, 4, 3]], dtype=int8)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "        block_size = 4\n",
        "        # Calculate the padded dimensions to make them multiples of the block size\n",
        "        padded_m = ((original_m + block_size - 1) // block_size) * block_size\n",
        "        padded_k = ((original_k1 + block_size - 1) // block_size) * block_size\n",
        "        padded_n = ((original_n + block_size - 1) // block_size) * block_size"
      ],
      "metadata": {
        "id": "7r8TdqzjZB7q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_m, original_k1, original_n, padded_m, padded_k, padded_n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1m9ZlmGZRmz",
        "outputId": "a0de1fc7-6d94-4060-a97a-8c2ea5e63044"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 5, 6, 8, 8, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    def pad_matrix(matrix, target_shape):\n",
        "        \"\"\"\n",
        "        Pads the matrix to the target shape.\n",
        "        matrix: Input matrix to be padded\n",
        "        target_shape: (rows, cols) - shape to pad the matrix to\n",
        "        \"\"\"\n",
        "        original_shape = matrix.shape\n",
        "        padded_matrix = np.zeros(target_shape, dtype=matrix.dtype)\n",
        "        padded_matrix[:original_shape[0], :original_shape[1]] = matrix\n",
        "        return padded_matrix\n",
        "matrix_a_padded = pad_matrix(matrix_a, (padded_m, padded_k))\n",
        "matrix_b_padded = pad_matrix(matrix_b, (padded_k, padded_n))\n",
        "matrix_a_padded, matrix_b_padded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIevAZElZ3ub",
        "outputId": "c8579d3d-0a43-424e-ed8a-94ed16c90bc3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[8, 7, 1, 1, 3, 0, 0, 0],\n",
              "        [3, 4, 4, 9, 9, 0, 0, 0],\n",
              "        [2, 5, 5, 2, 8, 0, 0, 0],\n",
              "        [1, 8, 3, 5, 8, 0, 0, 0],\n",
              "        [5, 4, 9, 9, 6, 0, 0, 0],\n",
              "        [8, 8, 1, 4, 7, 0, 0, 0],\n",
              "        [6, 0, 0, 8, 7, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int8),\n",
              " array([[5, 6, 2, 1, 2, 7, 0, 0],\n",
              "        [1, 1, 7, 1, 9, 3, 0, 0],\n",
              "        [6, 9, 5, 3, 1, 3, 0, 0],\n",
              "        [0, 5, 5, 1, 0, 3, 0, 0],\n",
              "        [1, 6, 9, 8, 4, 3, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int8))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "        result_matrix_padded = np.zeros((padded_m, padded_n), dtype=np.int32)\n",
        "        result_matrix_padded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-ZJCGwQafQB",
        "outputId": "a2519bb2-92bc-41ba-a59d-f70f785ac3df"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "        for i in range(0, padded_m, block_size):\n",
        "            for j in range(0, padded_n, block_size):\n",
        "                for k in range(0, padded_k, block_size):\n",
        "                    # Extract blocks of size NxN (or smaller if near boundary)\n",
        "                    a_block = matrix_a_padded[i:i+block_size, k:k+block_size]\n",
        "                    b_block = matrix_b_padded[k:k+block_size, j:j+block_size]\n",
        "\n",
        "                    # Perform block multiplication\n",
        "                    partial_result = np.dot(a_block, b_block)\n",
        "\n",
        "                    # Accumulate the result in the final matrix\n",
        "                    result_matrix_padded[i:i+block_size, j:j+block_size] += partial_result.astype(np.int32)\n",
        "                    print (i,j,k, a_block, b_block, partial_result, result_matrix_padded)\n",
        "result_matrix_padded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHhFRnCba8Ha",
        "outputId": "c92f7682-8077-4918-e187-f3e467cee565"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0 0 [[8 7 1 1]\n",
            " [3 4 4 9]\n",
            " [2 5 5 2]\n",
            " [1 8 3 5]] [[5 6 2 1]\n",
            " [1 1 7 1]\n",
            " [6 9 5 3]\n",
            " [0 5 5 1]] [[ 53  69  75  19]\n",
            " [ 43 103  99  28]\n",
            " [ 45  72  74  24]\n",
            " [ 31  66  98  23]] [[ 53  69  75  19   0   0   0   0]\n",
            " [ 43 103  99  28   0   0   0   0]\n",
            " [ 45  72  74  24   0   0   0   0]\n",
            " [ 31  66  98  23   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0]]\n",
            "0 0 4 [[3 0 0 0]\n",
            " [9 0 0 0]\n",
            " [8 0 0 0]\n",
            " [8 0 0 0]] [[1 6 9 8]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]] [[ 3 18 27 24]\n",
            " [ 9 54 81 72]\n",
            " [ 8 48 72 64]\n",
            " [ 8 48 72 64]] [[ 56  87 102  43   0   0   0   0]\n",
            " [ 52 157 180 100   0   0   0   0]\n",
            " [ 53 120 146  88   0   0   0   0]\n",
            " [ 39 114 170  87   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0]]\n",
            "0 4 0 [[8 7 1 1]\n",
            " [3 4 4 9]\n",
            " [2 5 5 2]\n",
            " [1 8 3 5]] [[2 7 0 0]\n",
            " [9 3 0 0]\n",
            " [1 3 0 0]\n",
            " [0 3 0 0]] [[80 83  0  0]\n",
            " [46 72  0  0]\n",
            " [54 50  0  0]\n",
            " [77 55  0  0]] [[ 56  87 102  43  80  83   0   0]\n",
            " [ 52 157 180 100  46  72   0   0]\n",
            " [ 53 120 146  88  54  50   0   0]\n",
            " [ 39 114 170  87  77  55   0   0]\n",
            " [  0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0]]\n",
            "0 4 4 [[3 0 0 0]\n",
            " [9 0 0 0]\n",
            " [8 0 0 0]\n",
            " [8 0 0 0]] [[4 3 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]] [[12  9  0  0]\n",
            " [36 27  0  0]\n",
            " [32 24  0  0]\n",
            " [32 24  0  0]] [[ 56  87 102  43  92  92   0   0]\n",
            " [ 52 157 180 100  82  99   0   0]\n",
            " [ 53 120 146  88  86  74   0   0]\n",
            " [ 39 114 170  87 109  79   0   0]\n",
            " [  0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0]]\n",
            "4 0 0 [[5 4 9 9]\n",
            " [8 8 1 4]\n",
            " [6 0 0 8]\n",
            " [0 0 0 0]] [[5 6 2 1]\n",
            " [1 1 7 1]\n",
            " [6 9 5 3]\n",
            " [0 5 5 1]] [[  83  -96 -128   45]\n",
            " [  54   85   97   23]\n",
            " [  30   76   52   14]\n",
            " [   0    0    0    0]] [[  56   87  102   43   92   92    0    0]\n",
            " [  52  157  180  100   82   99    0    0]\n",
            " [  53  120  146   88   86   74    0    0]\n",
            " [  39  114  170   87  109   79    0    0]\n",
            " [  83  -96 -128   45    0    0    0    0]\n",
            " [  54   85   97   23    0    0    0    0]\n",
            " [  30   76   52   14    0    0    0    0]\n",
            " [   0    0    0    0    0    0    0    0]]\n",
            "4 0 4 [[6 0 0 0]\n",
            " [7 0 0 0]\n",
            " [7 0 0 0]\n",
            " [0 0 0 0]] [[1 6 9 8]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]] [[ 6 36 54 48]\n",
            " [ 7 42 63 56]\n",
            " [ 7 42 63 56]\n",
            " [ 0  0  0  0]] [[ 56  87 102  43  92  92   0   0]\n",
            " [ 52 157 180 100  82  99   0   0]\n",
            " [ 53 120 146  88  86  74   0   0]\n",
            " [ 39 114 170  87 109  79   0   0]\n",
            " [ 89 -60 -74  93   0   0   0   0]\n",
            " [ 61 127 160  79   0   0   0   0]\n",
            " [ 37 118 115  70   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0]]\n",
            "4 4 0 [[5 4 9 9]\n",
            " [8 8 1 4]\n",
            " [6 0 0 8]\n",
            " [0 0 0 0]] [[2 7 0 0]\n",
            " [9 3 0 0]\n",
            " [1 3 0 0]\n",
            " [0 3 0 0]] [[ 55 101   0   0]\n",
            " [ 89  95   0   0]\n",
            " [ 12  66   0   0]\n",
            " [  0   0   0   0]] [[ 56  87 102  43  92  92   0   0]\n",
            " [ 52 157 180 100  82  99   0   0]\n",
            " [ 53 120 146  88  86  74   0   0]\n",
            " [ 39 114 170  87 109  79   0   0]\n",
            " [ 89 -60 -74  93  55 101   0   0]\n",
            " [ 61 127 160  79  89  95   0   0]\n",
            " [ 37 118 115  70  12  66   0   0]\n",
            " [  0   0   0   0   0   0   0   0]]\n",
            "4 4 4 [[6 0 0 0]\n",
            " [7 0 0 0]\n",
            " [7 0 0 0]\n",
            " [0 0 0 0]] [[4 3 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]\n",
            " [0 0 0 0]] [[24 18  0  0]\n",
            " [28 21  0  0]\n",
            " [28 21  0  0]\n",
            " [ 0  0  0  0]] [[ 56  87 102  43  92  92   0   0]\n",
            " [ 52 157 180 100  82  99   0   0]\n",
            " [ 53 120 146  88  86  74   0   0]\n",
            " [ 39 114 170  87 109  79   0   0]\n",
            " [ 89 -60 -74  93  79 119   0   0]\n",
            " [ 61 127 160  79 117 116   0   0]\n",
            " [ 37 118 115  70  40  87   0   0]\n",
            " [  0   0   0   0   0   0   0   0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 56,  87, 102,  43,  92,  92,   0,   0],\n",
              "       [ 52, 157, 180, 100,  82,  99,   0,   0],\n",
              "       [ 53, 120, 146,  88,  86,  74,   0,   0],\n",
              "       [ 39, 114, 170,  87, 109,  79,   0,   0],\n",
              "       [ 89, -60, -74,  93,  79, 119,   0,   0],\n",
              "       [ 61, 127, 160,  79, 117, 116,   0,   0],\n",
              "       [ 37, 118, 115,  70,  40,  87,   0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.dot(matrix_a.astype(np.int32), matrix_b.astype(np.int32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiNO6hAHb4G-",
        "outputId": "df4a03a3-7ad0-4022-cfa0-70c665ae6679"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 56,  87, 102,  43,  92,  92],\n",
              "       [ 52, 157, 180, 100,  82,  99],\n",
              "       [ 53, 120, 146,  88,  86,  74],\n",
              "       [ 39, 114, 170,  87, 109,  79],\n",
              "       [ 89, 196, 182,  93,  79, 119],\n",
              "       [ 61, 127, 160,  79, 117, 116],\n",
              "       [ 37, 118, 115,  70,  40,  87]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P8R95bVcTfd",
        "outputId": "317b5a17-1920-4c34-f817-242ce92e8a95"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8, 7, 1, 1, 3],\n",
              "       [3, 4, 4, 9, 9],\n",
              "       [2, 5, 5, 2, 8],\n",
              "       [1, 8, 3, 5, 8],\n",
              "       [5, 4, 9, 9, 6],\n",
              "       [8, 8, 1, 4, 7],\n",
              "       [6, 0, 0, 8, 7]], dtype=int8)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkfio1zocZQJ",
        "outputId": "fe9f1cb9-3081-4f47-bf32-34cf9f122af0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5, 6, 2, 1, 2, 7],\n",
              "       [1, 1, 7, 1, 9, 3],\n",
              "       [6, 9, 5, 3, 1, 3],\n",
              "       [0, 5, 5, 1, 0, 3],\n",
              "       [1, 6, 9, 8, 4, 3]], dtype=int8)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Block Matrix Multiplier\n",
        "class BlockMatrixMultiplier:\n",
        "    def __init__(self, block_size, input_precision='int8', accumulator_precision='int32'):\n",
        "        self.block_size = block_size  # Block size NxN\n",
        "        self.input_precision = input_precision\n",
        "        self.accumulator_precision = accumulator_precision\n",
        "\n",
        "    def pad_matrix(self, matrix, target_shape):\n",
        "        \"\"\"\n",
        "        Pads the matrix to the target shape using the data type of the input matrix.\n",
        "        \"\"\"\n",
        "        original_shape = matrix.shape\n",
        "        padded_matrix = np.zeros(target_shape, dtype=matrix.dtype)\n",
        "        padded_matrix[:original_shape[0], :original_shape[1]] = matrix\n",
        "        return padded_matrix\n",
        "\n",
        "    def multiply_blocks(self, matrix_a, matrix_b):\n",
        "        \"\"\"\n",
        "        Simulates block matrix multiplication for quantized inputs with padding.\n",
        "        matrix_a: (MxK), matrix_b: (KxN), Block Size: (NxN)\n",
        "        \"\"\"\n",
        "        original_m, original_k1 = matrix_a.shape\n",
        "        original_k2, original_n = matrix_b.shape\n",
        "        print(\"hey\",original_k1,original_k2)\n",
        "\n",
        "        assert original_k1 == original_k2, \"Matrix dimensions do not match for multiplication.\"\n",
        "\n",
        "        # Calculate the padded dimensions to make them multiples of the block size\n",
        "        padded_m = ((original_m + self.block_size - 1) // self.block_size) * self.block_size\n",
        "        padded_k = ((original_k1 + self.block_size - 1) // self.block_size) * self.block_size\n",
        "        padded_n = ((original_n + self.block_size - 1) // self.block_size) * self.block_size\n",
        "\n",
        "        # Pad the matrices using their own data types\n",
        "        matrix_a_padded = self.pad_matrix(matrix_a, (padded_m, padded_k))\n",
        "        matrix_b_padded = self.pad_matrix(matrix_b, (padded_k, padded_n))\n",
        "\n",
        "        # Initialize the padded result matrix using accumulator's data type\n",
        "        result_matrix_padded = np.zeros((padded_m, padded_n), dtype=np.int32)\n",
        "\n",
        "        # Convert matrices to the specified precision (only if input_precision is not already correct)\n",
        "        matrix_a_padded = matrix_a_padded.astype(np.int8 if self.input_precision == 'int8' else np.float32)\n",
        "        matrix_b_padded = matrix_b_padded.astype(np.int8 if self.input_precision == 'int8' else np.float32)\n",
        "\n",
        "        # Block matrix multiplication\n",
        "        for i in range(0, padded_m, self.block_size):\n",
        "            for j in range(0, padded_n, self.block_size):\n",
        "                for k in range(0, padded_k, self.block_size):\n",
        "                    a_block = matrix_a_padded[i:i+self.block_size, k:k+self.block_size]\n",
        "                    b_block = matrix_b_padded[k:k+self.block_size, j:j+self.block_size]\n",
        "                    partial_result = np.dot(a_block, b_block)\n",
        "                    result_matrix_padded[i:i+self.block_size, j:j+self.block_size] += partial_result.astype(np.int32)\n",
        "\n",
        "        # Trim the result matrix back to the original size\n",
        "        result_matrix = result_matrix_padded[:original_m, :original_n]\n",
        "        return result_matrix\n",
        "\n",
        "\n",
        "# im2col function\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"\n",
        "    Rearranges image blocks into columns for convolution as matrix multiplication.\n",
        "    input_data: (H, W, C) input feature map or image\n",
        "    filter_h: Filter height\n",
        "    filter_w: Filter width\n",
        "    stride: Stride for the convolution\n",
        "    pad: Padding for the input\n",
        "    \"\"\"\n",
        "    H, W, C = input_data.shape  # Height, Width, Channels\n",
        "    out_h = (H + 2 * pad - filter_h) // stride + 1\n",
        "    out_w = (W + 2 * pad - filter_w) // stride + 1\n",
        "\n",
        "    # Add padding to the input data if required\n",
        "    img = np.pad(input_data, [(pad, pad), (pad, pad), (0, 0)], 'constant')\n",
        "\n",
        "    # Prepare the columns\n",
        "    col = np.zeros((filter_h * filter_w * C, out_h * out_w))\n",
        "\n",
        "    # Slide the filter over the image and collect patches\n",
        "    for y in range(0, out_h):\n",
        "        y_min = y * stride\n",
        "        y_max = y_min + filter_h\n",
        "\n",
        "        for x in range(0, out_w):\n",
        "            x_min = x * stride\n",
        "            x_max = x_min + filter_w\n",
        "\n",
        "            patch = img[y_min:y_max, x_min:x_max, :].reshape(-1)\n",
        "            col[:, y * out_w + x] = patch\n",
        "\n",
        "    return col\n",
        "\n",
        "\n",
        "# ResNet Block Simulator\n",
        "class ResNetBlockSimulator:\n",
        "    def __init__(self, block_size=4):\n",
        "        self.block_size = block_size\n",
        "        self.matrix_multiplier = BlockMatrixMultiplier(block_size)\n",
        "\n",
        "    def conv2d(self, input_tensor, kernel, stride=1, pad=1):\n",
        "        \"\"\"\n",
        "        Simulate a 2D convolution using matrix multiplication via im2col.\n",
        "        input_tensor: (H, W, C) input feature map or image\n",
        "        kernel: (filter_h, filter_w, C, out_channels) convolution kernel\n",
        "        \"\"\"\n",
        "        filter_h, filter_w, in_channels, out_channels = kernel.shape\n",
        "\n",
        "        # Apply im2col to the input data\n",
        "        input_col = im2col(input_tensor, filter_h, filter_w, stride=stride, pad=pad)\n",
        "\n",
        "        # Reshape the kernel into matrix form\n",
        "        kernel_col = kernel.reshape(filter_h * filter_w * in_channels, out_channels)\n",
        "\n",
        "        # Perform matrix multiplication using block matrix multiplier\n",
        "        output_col = self.matrix_multiplier.multiply_blocks(input_col, kernel_col)\n",
        "\n",
        "        # Reshape the output to the correct dimensions (H_out, W_out, out_channels)\n",
        "        output_h = (input_tensor.shape[0] + 2 * pad - filter_h) // stride + 1\n",
        "        output_w = (input_tensor.shape[1] + 2 * pad - filter_w) // stride + 1\n",
        "        output_tensor = output_col.reshape(output_h, output_w, out_channels)\n",
        "\n",
        "        return output_tensor\n",
        "\n",
        "    def resnet_block(self, input_tensor, conv_kernel1, conv_kernel2, stride=1):\n",
        "        \"\"\"\n",
        "        Simulates a ResNet block with two convolutional layers and a residual connection.\n",
        "        \"\"\"\n",
        "        # First convolution using im2col + matrix multiplication\n",
        "        conv1_output = self.conv2d(input_tensor, conv_kernel1, stride=stride)\n",
        "\n",
        "        # Second convolution\n",
        "        conv2_output = self.conv2d(conv1_output, conv_kernel2)\n",
        "\n",
        "        # Residual connection (adding the input to the output of the second conv layer)\n",
        "        residual_output = conv2_output + input_tensor  # Assuming input and output have the same shape\n",
        "        return residual_output\n",
        "\n",
        "\n",
        "# Verification for Block Matrix Multiplication\n",
        "def verify_block_matrix_multiplication(matrix_a, matrix_b, block_size):\n",
        "    # Initialize the block matrix multiplier\n",
        "    block_multiplier = BlockMatrixMultiplier(block_size, input_precision='int8', accumulator_precision='int32')\n",
        "\n",
        "    # Perform block matrix multiplication\n",
        "    block_result = block_multiplier.multiply_blocks(matrix_a, matrix_b)\n",
        "\n",
        "    # Perform standard matrix multiplication using numpy for verification\n",
        "    standard_result = np.dot(matrix_a, matrix_b)\n",
        "\n",
        "    # Verify correctness by comparing the results\n",
        "    if np.array_equal(block_result, standard_result):\n",
        "        print(\"Block Matrix Multiplication is correct!\")\n",
        "    else:\n",
        "        print(\"Block Matrix Multiplication is incorrect!\")\n",
        "        print(\"Block Matrix Result:\\n\", block_result)\n",
        "        print(\"Standard Matrix Result:\\n\", standard_result)\n",
        "\n",
        "\n",
        "# Example usage of the ResNetBlockSimulator with im2col and matrix multiplication checker\n",
        "if __name__ == \"__main__\":\n",
        "    # Test case for block matrix multiplication\n",
        "    matrix_a = np.random.randint(0, 2, size=(7, 5), dtype=np.int8)  # Not a multiple of block size\n",
        "    matrix_b = np.random.randint(0, 2, size=(5, 6), dtype=np.int8)  # Not a multiple of block size\n",
        "    block_size = 4\n",
        "\n",
        "    print(\"Verifying block matrix multiplication...\")\n",
        "    verify_block_matrix_multiplication(matrix_a, matrix_b, block_size)\n",
        "\n",
        "    # Test case for ResNet block simulation\n",
        "    print(\"\\nSimulating ResNet block...\")\n",
        "    # Input tensor (e.g., 224x224 image with 3 channels)\n",
        "    input_tensor = np.random.randint(0, 10, size=(224, 224, 3), dtype=np.int8)\n",
        "\n",
        "    # Convolution kernels (3x3 filters)\n",
        "    conv_kernel1 = np.random.randint(0, 10, size=(3, 3, 3, 64), dtype=np.int8)  # First conv layer\n",
        "    conv_kernel2 = np.random.randint(0, 10, size=(3, 3, 64, 64), dtype=np.int8)  # Second conv layer\n",
        "\n",
        "    # Initialize the ResNet block simulator\n",
        "    resnet_block_simulator = ResNetBlockSimulator(block_size=4)\n",
        "\n",
        "    # Simulate a ResNet block\n",
        "    output_tensor = resnet_block_simulator.resnet_block(input_tensor, conv_kernel1, conv_kernel2)\n",
        "    print(\"Output of the ResNet block:\\n\", output_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "lxE-EiVfh9MX",
        "outputId": "9d123323-9417-4b1d-b661-8d11bc08449d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying block matrix multiplication...\n",
            "hey 5 5\n",
            "Block Matrix Multiplication is correct!\n",
            "\n",
            "Simulating ResNet block...\n",
            "hey 50176 27\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Matrix dimensions do not match for multiplication.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-9af5bbfc35f2>\u001b[0m in \u001b[0;36m<cell line: 161>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# Simulate a ResNet block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet_block_simulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_kernel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_kernel2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output of the ResNet block:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-9af5bbfc35f2>\u001b[0m in \u001b[0;36mresnet_block\u001b[0;34m(self, input_tensor, conv_kernel1, conv_kernel2, stride)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \"\"\"\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# First convolution using im2col + matrix multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mconv1_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_kernel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# Second convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-9af5bbfc35f2>\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(self, input_tensor, kernel, stride, pad)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Perform matrix multiplication using block matrix multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0moutput_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_multiplier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Reshape the output to the correct dimensions (H_out, W_out, out_channels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-9af5bbfc35f2>\u001b[0m in \u001b[0;36mmultiply_blocks\u001b[0;34m(self, matrix_a, matrix_b)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hey\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moriginal_k1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moriginal_k2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0moriginal_k1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moriginal_k2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Matrix dimensions do not match for multiplication.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Calculate the padded dimensions to make them multiples of the block size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Matrix dimensions do not match for multiplication."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# im2col function for converting the input into columns\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"\n",
        "    Rearranges image blocks into columns for convolution as matrix multiplication.\n",
        "    input_data: (H, W, C) input feature map or image\n",
        "    filter_h: Filter height\n",
        "    filter_w: Filter width\n",
        "    stride: Stride for the convolution\n",
        "    pad: Padding for the input\n",
        "    \"\"\"\n",
        "    H, W, C = input_data.shape  # Height, Width, Channels\n",
        "    out_h = (H + 2 * pad - filter_h) // stride + 1\n",
        "    out_w = (W + 2 * pad - filter_w) // stride + 1\n",
        "\n",
        "    # Add padding to the input data if required\n",
        "    img = np.pad(input_data, [(pad, pad), (pad, pad), (0, 0)], 'constant')\n",
        "\n",
        "    # Prepare the columns\n",
        "    col = np.zeros((filter_h * filter_w * C, out_h * out_w))\n",
        "\n",
        "    # Slide the filter over the image and collect patches\n",
        "    for y in range(0, out_h):\n",
        "        y_min = y * stride\n",
        "        y_max = y_min + filter_h\n",
        "\n",
        "        for x in range(0, out_w):\n",
        "            x_min = x * stride\n",
        "            x_max = x_min + filter_w\n",
        "\n",
        "            patch = img[y_min:y_max, x_min:x_max, :].reshape(-1)\n",
        "            col[:, y * out_w + x] = patch\n",
        "\n",
        "    return col\n",
        "\n",
        "# Input image of size (224, 224, 3)\n",
        "input_image = np.random.randn(224, 224, 3).astype(np.float32)\n",
        "\n",
        "# 7x7 filter with 3 input channels (RGB) and 64 output channels\n",
        "filter_h, filter_w, in_channels, out_channels = 7, 7, 3, 64\n",
        "conv_kernel = np.random.randn(filter_h, filter_w, in_channels, out_channels).astype(np.float32)\n",
        "\n",
        "# Convert the input image to column format (im2col)\n",
        "input_col = im2col(input_image, filter_h, filter_w, stride=2, pad=3)\n",
        "\n",
        "# Reshape the convolutional kernel into a matrix for multiplication\n",
        "conv_kernel_col = conv_kernel.reshape(filter_h * filter_w * in_channels, out_channels)\n",
        "\n",
        "# Perform matrix multiplication\n",
        "output_col = np.dot(input_col.T, conv_kernel_col)\n",
        "\n",
        "# Reshape the output back into the correct output dimensions\n",
        "output_h = (input_image.shape[0] + 2 * 3 - filter_h) // 2 + 1\n",
        "output_w = (input_image.shape[1] + 2 * 3 - filter_w) // 2 + 1\n",
        "output = output_col.reshape(output_h, output_w, out_channels)\n",
        "\n",
        "print(\"Output shape:\", output.shape)  # Should be (112, 112, 64)\n",
        "\n",
        "print(\"input shape:\", input_col.shape)  # Should be (147, 12544)\n",
        "\n",
        "print(\"kernel shape:\", conv_kernel_col.shape)  # Should be (147, 12544)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLOhS04fm9TN",
        "outputId": "4e03fac8-f44b-4d96-de6d-8e322ff5d9ba"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (112, 112, 64)\n",
            "input shape: (147, 12544)\n",
            "kernel shape: (147, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# im2col function for converting the input into columns\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"\n",
        "    Rearranges image blocks into columns for convolution as matrix multiplication.\n",
        "    input_data: (H, W, C) input feature map or image\n",
        "    filter_h: Filter height\n",
        "    filter_w: Filter width\n",
        "    stride: Stride for the convolution\n",
        "    pad: Padding for the input\n",
        "    \"\"\"\n",
        "    H, W, C = input_data.shape  # Height, Width, Channels\n",
        "    out_h = (H + 2 * pad - filter_h) // stride + 1\n",
        "    out_w = (W + 2 * pad - filter_w) // stride + 1\n",
        "\n",
        "    # Add padding to the input data if required\n",
        "    img = np.pad(input_data, [(pad, pad), (pad, pad), (0, 0)], 'constant')\n",
        "\n",
        "    # Prepare the columns\n",
        "    col = np.zeros((filter_h * filter_w * C, out_h * out_w))\n",
        "\n",
        "    # Slide the filter over the image and collect patches\n",
        "    for y in range(0, out_h):\n",
        "        y_min = y * stride\n",
        "        y_max = y_min + filter_h\n",
        "\n",
        "        for x in range(0, out_w):\n",
        "            x_min = x * stride\n",
        "            x_max = x_min + filter_w\n",
        "\n",
        "            patch = img[y_min:y_max, x_min:x_max, :].reshape(-1)\n",
        "            col[:, y * out_w + x] = patch\n",
        "\n",
        "    return col\n",
        "\n",
        "# Convolution layer\n",
        "def conv_layer(input_tensor, conv_kernel, stride=1, pad=1):\n",
        "    filter_h, filter_w, in_channels, out_channels = conv_kernel.shape\n",
        "\n",
        "    # Convert input image to column format (im2col)\n",
        "    input_col = im2col(input_tensor, filter_h, filter_w, stride=stride, pad=pad)\n",
        "\n",
        "    # Reshape convolution kernel into matrix format\n",
        "    conv_kernel_col = conv_kernel.reshape(filter_h * filter_w * in_channels, out_channels)\n",
        "\n",
        "    # Perform matrix multiplication\n",
        "    output_col = np.dot(input_col.T, conv_kernel_col)\n",
        "\n",
        "    # Reshape output back into the correct dimensions\n",
        "    output_h = (input_tensor.shape[0] + 2 * pad - filter_h) // stride + 1\n",
        "    output_w = (input_tensor.shape[1] + 2 * pad - filter_w) // stride + 1\n",
        "    output = output_col.reshape(output_h, output_w, out_channels)\n",
        "\n",
        "    return output\n",
        "\n",
        "# Max pooling layer\n",
        "def max_pooling(input_data, kernel_size=3, stride=2, pad=1):\n",
        "    H, W, C = input_data.shape\n",
        "    out_h = (H + 2 * pad - kernel_size) // stride + 1\n",
        "    out_w = (W + 2 * pad - kernel_size) // stride + 1\n",
        "\n",
        "    # Add padding\n",
        "    input_data_padded = np.pad(input_data, [(pad, pad), (pad, pad), (0, 0)], mode='constant')\n",
        "\n",
        "    # Initialize output\n",
        "    pooled_output = np.zeros((out_h, out_w, C))\n",
        "\n",
        "    # Perform max pooling\n",
        "    for c in range(C):\n",
        "        for y in range(out_h):\n",
        "            y_min = y * stride\n",
        "            y_max = y_min + kernel_size\n",
        "            for x in range(out_w):\n",
        "                x_min = x * stride\n",
        "                x_max = x_min + kernel_size\n",
        "                pooled_output[y, x, c] = np.max(input_data_padded[y_min:y_max, x_min:x_max, c])\n",
        "\n",
        "    return pooled_output\n",
        "\n",
        "# Residual block with two 3x3 convolution layers and a 1x1 downsample if needed\n",
        "def resnet18_residual_block(input_tensor, in_channels, out_channels, stride=1):\n",
        "    # First 3x3 convolution\n",
        "    conv_kernel1 = np.random.randn(3, 3, in_channels, out_channels).astype(np.float32)\n",
        "    output1 = conv_layer(input_tensor, conv_kernel1, stride=stride, pad=1)\n",
        "\n",
        "    # Second 3x3 convolution\n",
        "    conv_kernel2 = np.random.randn(3, 3, out_channels, out_channels).astype(np.float32)\n",
        "    output2 = conv_layer(output1, conv_kernel2, stride=1, pad=1)\n",
        "\n",
        "    # Check if the input shape and output shape are the same\n",
        "    if input_tensor.shape != output2.shape:\n",
        "        # Perform downsampling with 1x1 convolution if shapes differ\n",
        "        downsample_kernel = np.random.randn(1, 1, in_channels, out_channels).astype(np.float32)\n",
        "        input_resized = conv_layer(input_tensor, downsample_kernel, stride=stride, pad=0)\n",
        "    else:\n",
        "        input_resized = input_tensor\n",
        "\n",
        "    # Add the residual connection (input_tensor added to the final output)\n",
        "    residual_output = output2 + input_resized\n",
        "\n",
        "    return residual_output\n",
        "\n",
        "# Global Average Pooling\n",
        "def global_avg_pooling(input_tensor):\n",
        "    return np.mean(input_tensor, axis=(0, 1))  # Average over height and width\n",
        "\n",
        "# Fully Connected Layer (FC)\n",
        "def fully_connected(input_vector, fc_weights):\n",
        "    return np.dot(input_vector, fc_weights)\n",
        "\n",
        "# ResNet-18 architecture, layer by layer\n",
        "def resnet18_forward(input_image):\n",
        "    # Initial convolution layer (7x7, stride 2, padding 3, 64 output channels)\n",
        "    conv_kernel_init = np.random.randn(7, 7, 3, 64).astype(np.float32)\n",
        "    output = conv_layer(input_image, conv_kernel_init, stride=2, pad=3)\n",
        "    print(\"Initial Conv Output Shape:\", output.shape)\n",
        "\n",
        "    # Max pooling layer (3x3, stride 2, padding 1)\n",
        "    output = max_pooling(output)\n",
        "    print(\"Max Pool Output Shape:\", output.shape)\n",
        "\n",
        "    # Residual Block Stage 1 (2 blocks, 64 channels)\n",
        "    output = resnet18_residual_block(output, 64, 64, stride=1)\n",
        "    output = resnet18_residual_block(output, 64, 64, stride=1)\n",
        "    print(\"Stage 1 Output Shape:\", output.shape)\n",
        "\n",
        "    # Residual Block Stage 2 (2 blocks, 128 channels, first block with stride 2)\n",
        "    output = resnet18_residual_block(output, 64, 128, stride=2)\n",
        "    output = resnet18_residual_block(output, 128, 128, stride=1)\n",
        "    print(\"Stage 2 Output Shape:\", output.shape)\n",
        "\n",
        "    # Residual Block Stage 3 (2 blocks, 256 channels, first block with stride 2)\n",
        "    output = resnet18_residual_block(output, 128, 256, stride=2)\n",
        "    output = resnet18_residual_block(output, 256, 256, stride=1)\n",
        "    print(\"Stage 3 Output Shape:\", output.shape)\n",
        "\n",
        "    # Residual Block Stage 4 (2 blocks, 512 channels, first block with stride 2)\n",
        "    output = resnet18_residual_block(output, 256, 512, stride=2)\n",
        "    output = resnet18_residual_block(output, 512, 512, stride=1)\n",
        "    print(\"Stage 4 Output Shape:\", output.shape)\n",
        "\n",
        "    # Global Average Pooling\n",
        "    output = global_avg_pooling(output)\n",
        "    print(\"Global Average Pooling Output Shape:\", output.shape)\n",
        "\n",
        "    # Fully connected layer\n",
        "    fc_weights = np.random.randn(512, 1000).astype(np.float32)  # 1000 output classes\n",
        "    output = fully_connected(output, fc_weights)\n",
        "    print(\"Fully Connected Output Shape:\", output.shape)\n",
        "\n",
        "    return output\n",
        "\n",
        "# Example usage of ResNet-18 architecture\n",
        "input_image = np.random.randn(224, 224, 3).astype(np.float32)  # Input image of size (224, 224, 3)\n",
        "output_resnet18 = resnet18_forward(input_image)\n",
        "print(\"Final Output Shape:\", output_resnet18.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfJEWy2notpS",
        "outputId": "d87a41d6-e03e-4153-9fc6-b74a9b64368e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Conv Output Shape: (112, 112, 64)\n",
            "Max Pool Output Shape: (56, 56, 64)\n",
            "Stage 1 Output Shape: (56, 56, 64)\n",
            "Stage 2 Output Shape: (28, 28, 128)\n",
            "Stage 3 Output Shape: (14, 14, 256)\n",
            "Stage 4 Output Shape: (7, 7, 512)\n",
            "Global Average Pooling Output Shape: (512,)\n",
            "Fully Connected Output Shape: (1000,)\n",
            "Final Output Shape: (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Block Matrix Multiplier\n",
        "class BlockMatrixMultiplier:\n",
        "    def __init__(self, block_size, input_precision='int8', accumulator_precision='int32'):\n",
        "        self.block_size = block_size  # Block size NxN\n",
        "        self.input_precision = input_precision\n",
        "        self.accumulator_precision = accumulator_precision\n",
        "\n",
        "    def pad_matrix(self, matrix, target_shape):\n",
        "        \"\"\"\n",
        "        Pads the matrix to the target shape using the data type of the input matrix.\n",
        "        \"\"\"\n",
        "        original_shape = matrix.shape\n",
        "        padded_matrix = np.zeros(target_shape, dtype=matrix.dtype)\n",
        "        padded_matrix[:original_shape[0], :original_shape[1]] = matrix\n",
        "        return padded_matrix\n",
        "\n",
        "    def multiply_blocks(self, matrix_a, matrix_b):\n",
        "        \"\"\"\n",
        "        Simulates block matrix multiplication for quantized inputs with padding.\n",
        "        matrix_a: (MxK), matrix_b: (KxN), Block Size: (NxN)\n",
        "        \"\"\"\n",
        "        original_m, original_k1 = matrix_a.shape\n",
        "        original_k2, original_n = matrix_b.shape\n",
        "\n",
        "        assert original_k1 == original_k2, \"Matrix dimensions do not match for multiplication.\"\n",
        "\n",
        "        # Calculate the padded dimensions to make them multiples of the block size\n",
        "        padded_m = ((original_m + self.block_size - 1) // self.block_size) * self.block_size\n",
        "        padded_k = ((original_k1 + self.block_size - 1) // self.block_size) * self.block_size\n",
        "        padded_n = ((original_n + self.block_size - 1) // self.block_size) * self.block_size\n",
        "\n",
        "        # Pad the matrices using their own data types\n",
        "        matrix_a_padded = self.pad_matrix(matrix_a, (padded_m, padded_k))\n",
        "        matrix_b_padded = self.pad_matrix(matrix_b, (padded_k, padded_n))\n",
        "\n",
        "        # Initialize the padded result matrix using accumulator's data type\n",
        "        result_matrix_padded = np.zeros((padded_m, padded_n), dtype=np.int32)\n",
        "\n",
        "        # Convert matrices to the specified precision (only if input_precision is not already correct)\n",
        "        matrix_a_padded = matrix_a_padded.astype(np.int8 if self.input_precision == 'int8' else np.float32)\n",
        "        matrix_b_padded = matrix_b_padded.astype(np.int8 if self.input_precision == 'int8' else np.float32)\n",
        "\n",
        "        # Block matrix multiplication\n",
        "        for i in range(0, padded_m, self.block_size):\n",
        "            for j in range(0, padded_n, self.block_size):\n",
        "                for k in range(0, padded_k, self.block_size):\n",
        "                    a_block = matrix_a_padded[i:i+self.block_size, k:k+self.block_size]\n",
        "                    b_block = matrix_b_padded[k:k+self.block_size, j:j+self.block_size]\n",
        "                    partial_result = np.dot(a_block, b_block)\n",
        "                    result_matrix_padded[i:i+self.block_size, j:j+self.block_size] += partial_result.astype(np.int32)\n",
        "\n",
        "        # Trim the result matrix back to the original size\n",
        "        result_matrix = result_matrix_padded[:original_m, :original_n]\n",
        "        return result_matrix\n",
        "\n",
        "\n",
        "# im2col function for converting the input into columns\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"\n",
        "    Rearranges image blocks into columns for convolution as matrix multiplication.\n",
        "    input_data: (H, W, C) input feature map or image\n",
        "    filter_h: Filter height\n",
        "    filter_w: Filter width\n",
        "    stride: Stride for the convolution\n",
        "    pad: Padding for the input\n",
        "    \"\"\"\n",
        "    H, W, C = input_data.shape  # Height, Width, Channels\n",
        "    out_h = (H + 2 * pad - filter_h) // stride + 1\n",
        "    out_w = (W + 2 * pad - filter_w) // stride + 1\n",
        "\n",
        "    # Add padding to the input data if required\n",
        "    img = np.pad(input_data, [(pad, pad), (pad, pad), (0, 0)], 'constant')\n",
        "\n",
        "    # Prepare the columns\n",
        "    col = np.zeros((filter_h * filter_w * C, out_h * out_w))\n",
        "\n",
        "    # Slide the filter over the image and collect patches\n",
        "    for y in range(0, out_h):\n",
        "        y_min = y * stride\n",
        "        y_max = y_min + filter_h\n",
        "\n",
        "        for x in range(0, out_w):\n",
        "            x_min = x * stride\n",
        "            x_max = x_min + filter_w\n",
        "\n",
        "            patch = img[y_min:y_max, x_min:x_max, :].reshape(-1)\n",
        "            col[:, y * out_w + x] = patch\n",
        "\n",
        "    return col\n",
        "\n",
        "# Convolution layer with block matrix multiplication\n",
        "def conv_layer(input_tensor, conv_kernel, block_multiplier, stride=1, pad=1):\n",
        "    filter_h, filter_w, in_channels, out_channels = conv_kernel.shape\n",
        "\n",
        "    # Convert input image to column format (im2col)\n",
        "    input_col = im2col(input_tensor, filter_h, filter_w, stride=stride, pad=pad)\n",
        "\n",
        "    # Reshape convolution kernel into matrix format\n",
        "    conv_kernel_col = conv_kernel.reshape(filter_h * filter_w * in_channels, out_channels)\n",
        "\n",
        "    # Perform matrix multiplication using the block matrix multiplier\n",
        "    output_col = block_multiplier.multiply_blocks(input_col.T, conv_kernel_col)\n",
        "\n",
        "    # Reshape output back into the correct dimensions\n",
        "    output_h = (input_tensor.shape[0] + 2 * pad - filter_h) // stride + 1\n",
        "    output_w = (input_tensor.shape[1] + 2 * pad - filter_w) // stride + 1\n",
        "    output = output_col.reshape(output_h, output_w, out_channels)\n",
        "\n",
        "    return output\n",
        "\n",
        "# Max pooling layer\n",
        "def max_pooling(input_data, kernel_size=3, stride=2, pad=1):\n",
        "    H, W, C = input_data.shape\n",
        "    out_h = (H + 2 * pad - kernel_size) // stride + 1\n",
        "    out_w = (W + 2 * pad - kernel_size) // stride + 1\n",
        "\n",
        "    # Add padding\n",
        "    input_data_padded = np.pad(input_data, [(pad, pad), (pad, pad), (0, 0)], mode='constant')\n",
        "\n",
        "    # Initialize output\n",
        "    pooled_output = np.zeros((out_h, out_w, C))\n",
        "\n",
        "    # Perform max pooling\n",
        "    for c in range(C):\n",
        "        for y in range(out_h):\n",
        "            y_min = y * stride\n",
        "            y_max = y_min + kernel_size\n",
        "            for x in range(out_w):\n",
        "                x_min = x * stride\n",
        "                x_max = x_min + kernel_size\n",
        "                pooled_output[y, x, c] = np.max(input_data_padded[y_min:y_max, x_min:x_max, c])\n",
        "\n",
        "    return pooled_output\n",
        "\n",
        "# Dedicated Functional Unit (DFU) for element-wise operations (e.g., addition)\n",
        "class DFU:\n",
        "    @staticmethod\n",
        "    def add(tensor_a, tensor_b):\n",
        "        return tensor_a + tensor_b\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(tensor):\n",
        "        return np.maximum(0, tensor)\n",
        "\n",
        "# Residual block with two 3x3 convolution layers and a 1x1 downsample if needed\n",
        "def resnet18_residual_block(input_tensor, in_channels, out_channels, block_multiplier, stride=1):\n",
        "    # First 3x3 convolution\n",
        "    conv_kernel1 = np.random.randn(3, 3, in_channels, out_channels).astype(np.float32)\n",
        "    output1 = conv_layer(input_tensor, conv_kernel1, block_multiplier, stride=stride, pad=1)\n",
        "\n",
        "    # Second 3x3 convolution\n",
        "    conv_kernel2 = np.random.randn(3, 3, out_channels, out_channels).astype(np.float32)\n",
        "    output2 = conv_layer(output1, conv_kernel2, block_multiplier, stride=1, pad=1)\n",
        "\n",
        "    # Check if the input shape and output shape are the same\n",
        "    if input_tensor.shape != output2.shape:\n",
        "        # Perform downsampling with 1x1 convolution if shapes differ\n",
        "        downsample_kernel = np.random.randn(1, 1, in_channels, out_channels).astype(np.float32)\n",
        "        input_resized = conv_layer(input_tensor, downsample_kernel, block_multiplier, stride=stride, pad=0)\n",
        "    else:\n",
        "        input_resized = input_tensor\n",
        "\n",
        "    # Add the residual connection (input_tensor added to the final output)\n",
        "    residual_output = DFU.add(output2, input_resized)\n",
        "    return DFU.relu(residual_output)\n",
        "\n",
        "# Global Average Pooling\n",
        "def global_avg_pooling(input_tensor):\n",
        "    return np.mean(input_tensor, axis=(0, 1))  # Average over height and width\n",
        "\n",
        "# Fully Connected Layer (FC)\n",
        "def fully_connected(input_vector, fc_weights):\n",
        "    return np.dot(input_vector, fc_weights)\n",
        "\n",
        "# ResNet-18 architecture, layer by layer\n",
        "def resnet18_forward(input_image, block_multiplier):\n",
        "    # Initial convolution layer (7x7, stride 2, padding 3, 64 output channels)\n",
        "    conv_kernel_init = np.random.randn(7, 7, 3, 64).astype(np.float32)\n",
        "    output = conv_layer(input_image, conv_kernel_init, block_multiplier, stride=2, pad=3)\n",
        "    print(\"Initial Conv Output Shape:\", output.shape)\n",
        "\n",
        "    # Max pooling layer (3x3, stride 2, padding 1)\n",
        "    output = max_pooling(output)\n",
        "    print(\"Max Pool Output Shape:\", output.shape)\n",
        "\n",
        "    # Residual Block Stage 1 (2 blocks, 64 channels)\n",
        "    output = resnet18_residual_block(output, 64, 64, block_multiplier, stride=1)\n",
        "    output = resnet18_residual_block(output, 64, 64, block_multiplier, stride=1)\n",
        "    print(\"Stage 1 Output Shape:\", output.shape)\n",
        "\n",
        "    # Residual Block Stage 2 (2 blocks, 128 channels, first block with stride 2)\n",
        "    output = resnet18_residual_block(output, 64, 128, block_multiplier, stride=2)\n",
        "    output = resnet18_residual_block(output, 128, 128, block_multiplier, stride=1)\n",
        "    print(\"Stage 2 Output Shape:\", output.shape)\n",
        "\n",
        "    # Residual Block Stage 3 (2 blocks, 256 channels, first block with stride 2)\n",
        "    output = resnet18_residual_block(output, 128, 256, block_multiplier, stride=2)\n",
        "    output = resnet18_residual_block(output, 256, 256, block_multiplier, stride=1)\n",
        "    print(\"Stage 3 Output Shape:\", output.shape)\n",
        "\n",
        "    # Residual Block Stage 4 (2 blocks, 512 channels, first block with stride 2)\n",
        "    output = resnet18_residual_block(output, 256, 512, block_multiplier, stride=2)\n",
        "    output = resnet18_residual_block(output, 512, 512, block_multiplier, stride=1)\n",
        "    print(\"Stage 4 Output Shape:\", output.shape)\n",
        "\n",
        "    # Global Average Pooling\n",
        "    output = global_avg_pooling(output)\n",
        "    print(\"Global Average Pooling Output Shape:\", output.shape)\n",
        "\n",
        "    # Fully connected layer\n",
        "    fc_weights = np.random.randn(512, 1000).astype(np.float32)  # 1000 output classes\n",
        "    output = fully_connected(output, fc_weights)\n",
        "    print(\"Fully Connected Output Shape:\", output.shape)\n",
        "\n",
        "    return output\n",
        "\n",
        "# Example usage of ResNet-18 architecture with block matrix multiplication\n",
        "block_multiplier = BlockMatrixMultiplier(block_size=4)\n",
        "input_image = np.random.randn(224, 224, 3).astype(np.float32)  # Input image of size (224, 224, 3)\n",
        "output_resnet18 = resnet18_forward(input_image, block_multiplier)\n",
        "print(\"Final Output Shape:\", output_resnet18.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "N93U1eSirghZ",
        "outputId": "f2c88177-4072-40cb-e64b-141b2f5516b5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-0907148744cd>\u001b[0m in \u001b[0;36m<cell line: 222>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0mblock_multiplier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlockMatrixMultiplier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0minput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Input image of size (224, 224, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m \u001b[0moutput_resnet18\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet18_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_multiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final Output Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_resnet18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-0907148744cd>\u001b[0m in \u001b[0;36mresnet18_forward\u001b[0;34m(input_image, block_multiplier)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;31m# Initial convolution layer (7x7, stride 2, padding 3, 64 output channels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mconv_kernel_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_kernel_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_multiplier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial Conv Output Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-0907148744cd>\u001b[0m in \u001b[0;36mconv_layer\u001b[0;34m(input_tensor, conv_kernel, block_multiplier, stride, pad)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# Perform matrix multiplication using the block matrix multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0moutput_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_multiplier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_col\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_kernel_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# Reshape output back into the correct dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-0907148744cd>\u001b[0m in \u001b[0;36mmultiply_blocks\u001b[0;34m(self, matrix_a, matrix_b)\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mb_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_b_padded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0mpartial_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_block\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_block\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                     \u001b[0mresult_matrix_padded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpartial_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Trim the result matrix back to the original size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the full transcript of the questions asked for the session today\n",
        "\n",
        "Apologies for the confusion. Let me provide a more comprehensive transcript, including the missing questions. Here’s the full list of questions you’ve asked throughout this session:\n",
        "\n",
        "\t1.\tPls write detailed technical feature requirements for a hardware matrix multiplier for quantized neural network inference computation.\n",
        "\t2.\tAlso add requirements to support the other PyTorch tensor operators which might not be supported through matrix multiplication. Do this through a dedicated functional unit for such purpose.\n",
        "\t3.\tFor each requirements that you created - can you write a functional simulator model. Use any language that’s suitable.\n",
        "\t4.\tLet us modify the multiplier to a block multiplier of NxN.\n",
        "\t5.\tAdd code to confirm if the matrix multiplication result was correct.\n",
        "\t6.\tOK. Put all the functional models together in one script.\n",
        "\t7.\tHow would a resnet be mapped on this functional simulator?\n",
        "\t8.\tConsolidate the full functional model including the resnet simulator code in one script.\n",
        "\t9.\tUpgrade the matrix multiplication functional code to extend the input matrix sizes to a multiple of matrix multiplier block sizes - but output only the dimensions that are needed.\n",
        "\t10.\tUpgrade the full script with this improved version and run it to check for correctness for matrix multiplication and resnet example.\n",
        "\t11.\tNow expand the script to include the actual resnet computation for resnet18.\n",
        "\t12.\tDo not delete the matrix multiply checker code from the script. Add it back.\n",
        "\t13.\tNow expand the script to include the actual resnet computation for resnet18.\n",
        "\t14.\tLet us first start with how to convert the resnet18 computation to matrix multiplications - layer by layer. Start with an example.\n",
        "\t15.\tOK. Now convert the resnet18 layer by layer to the matrix style multiplication.\n",
        "\t16.\tAssemble the python code that converts resnet18 to matrix multiplication for all layers.\n",
        "\t17.\tInput_2 and output_resized are not the same shape in this code. Something is wrong. What is it?\n",
        "\t18.\tNow integrate this whole code into the matrix multiplication and the DFU codebase.\n",
        "\t19.\tPlease give me the full transcript of all the questions that I asked.\n",
        "\n",
        "I hope this is now complete. Let me know if you need further clarifications or adjustments!"
      ],
      "metadata": {
        "id": "UgMzfQK-sx1P"
      }
    }
  ]
}