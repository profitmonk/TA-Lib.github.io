{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/profitmonk/TA-Lib.github.io/blob/main/ConvNext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp15VLnAODcC",
        "outputId": "021279b7-1b17-40d6-a05d-6f762917365b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169001437/169001437 [00:18<00:00, 9268793.02it/s] \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n",
            "100%|██████████| 109M/109M [00:00<00:00, 192MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "[100] loss: 4.473 | acc: 3.33%\n",
            "[200] loss: 4.134 | acc: 4.83%\n",
            "[300] loss: 3.989 | acc: 6.08%\n",
            "[400] loss: 3.827 | acc: 7.33%\n",
            "[500] loss: 3.706 | acc: 8.30%\n",
            "[600] loss: 3.570 | acc: 9.32%\n",
            "[700] loss: 3.439 | acc: 10.49%\n",
            "Training completed in 1407.19 seconds\n",
            "Final training accuracy: 11.29%\n",
            "Test accuracy: 18.77%\n",
            "Starting training...\n",
            "[100] loss: 3.238 | acc: 19.89%\n",
            "[200] loss: 3.171 | acc: 21.02%\n",
            "[300] loss: 3.120 | acc: 21.48%\n",
            "[400] loss: 3.024 | acc: 22.00%\n",
            "[500] loss: 2.966 | acc: 22.79%\n",
            "[600] loss: 2.897 | acc: 23.58%\n",
            "[700] loss: 2.846 | acc: 24.20%\n",
            "Epoch 1/5\n",
            "Training completed in 1405.06 seconds\n",
            "Training accuracy: 24.78%\n",
            "Test accuracy: 30.34%\n",
            "Checkpoint saved: checkpoints/convnext_epoch_1.pth\n",
            "-----------------------------\n",
            "[100] loss: 2.568 | acc: 33.67%\n",
            "[200] loss: 2.551 | acc: 33.84%\n",
            "[300] loss: 2.551 | acc: 34.08%\n",
            "[400] loss: 2.496 | acc: 34.25%\n",
            "[500] loss: 2.458 | acc: 34.42%\n",
            "[600] loss: 2.430 | acc: 34.72%\n",
            "[700] loss: 2.443 | acc: 34.89%\n",
            "Epoch 2/5\n",
            "Training completed in 1405.75 seconds\n",
            "Training accuracy: 35.16%\n",
            "Test accuracy: 38.36%\n",
            "Checkpoint saved: checkpoints/convnext_epoch_2.pth\n",
            "-----------------------------\n",
            "[100] loss: 2.048 | acc: 44.66%\n",
            "[200] loss: 2.042 | acc: 44.02%\n",
            "[300] loss: 2.034 | acc: 44.46%\n",
            "[400] loss: 2.070 | acc: 44.34%\n",
            "[500] loss: 2.010 | acc: 44.73%\n",
            "[600] loss: 1.953 | acc: 45.11%\n",
            "[700] loss: 1.934 | acc: 45.35%\n",
            "Epoch 3/5\n",
            "Training completed in 1406.86 seconds\n",
            "Training accuracy: 45.50%\n",
            "Test accuracy: 41.05%\n",
            "Checkpoint saved: checkpoints/convnext_epoch_3.pth\n",
            "-----------------------------\n",
            "[100] loss: 1.455 | acc: 59.03%\n",
            "[200] loss: 1.455 | acc: 59.08%\n",
            "[300] loss: 1.500 | acc: 58.65%\n",
            "[400] loss: 1.598 | acc: 57.91%\n",
            "[500] loss: 1.525 | acc: 57.65%\n",
            "[600] loss: 1.529 | acc: 57.28%\n",
            "[700] loss: 1.545 | acc: 57.09%\n",
            "Epoch 4/5\n",
            "Training completed in 1407.35 seconds\n",
            "Training accuracy: 57.03%\n",
            "Test accuracy: 47.66%\n",
            "Checkpoint saved: checkpoints/convnext_epoch_4.pth\n",
            "-----------------------------\n",
            "[100] loss: 0.839 | acc: 75.75%\n",
            "[200] loss: 0.819 | acc: 75.79%\n",
            "[300] loss: 0.860 | acc: 75.22%\n",
            "[400] loss: 0.893 | acc: 74.60%\n",
            "[500] loss: 0.987 | acc: 73.67%\n",
            "[600] loss: 0.974 | acc: 73.14%\n",
            "[700] loss: 1.004 | acc: 72.72%\n",
            "Epoch 5/5\n",
            "Training completed in 1406.55 seconds\n",
            "Training accuracy: 72.47%\n",
            "Test accuracy: 47.46%\n",
            "Checkpoint saved: checkpoints/convnext_epoch_5.pth\n",
            "-----------------------------\n",
            "Training finished.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "from torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights\n",
        "\n",
        "# ConvNeXt Tiny architecture (as defined previously)\n",
        "class ConvNeXtTiny(nn.Module):\n",
        "    def __init__(self, num_classes=100):  # Changed to 100 for CIFAR-100\n",
        "        super(ConvNeXtTiny, self).__init__()\n",
        "\n",
        "        # Stem layer\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=4, stride=4),\n",
        "            LayerNorm(96, eps=1e-6, data_format=\"channels_first\")\n",
        "        )\n",
        "\n",
        "        # Stage 1\n",
        "        self.stage1 = nn.Sequential(\n",
        "            ConvNeXtBlock(dim=96),\n",
        "            ConvNeXtBlock(dim=96),\n",
        "            ConvNeXtBlock(dim=96)\n",
        "        )\n",
        "\n",
        "        # Downsampling 1\n",
        "        self.downsample1 = nn.Sequential(\n",
        "            LayerNorm(96, eps=1e-6, data_format=\"channels_first\"),\n",
        "            nn.Conv2d(96, 192, kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Stage 2\n",
        "        self.stage2 = nn.Sequential(\n",
        "            ConvNeXtBlock(dim=192),\n",
        "            ConvNeXtBlock(dim=192),\n",
        "            ConvNeXtBlock(dim=192)\n",
        "        )\n",
        "\n",
        "        # Downsampling 2\n",
        "        self.downsample2 = nn.Sequential(\n",
        "            LayerNorm(192, eps=1e-6, data_format=\"channels_first\"),\n",
        "            nn.Conv2d(192, 384, kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Stage 3\n",
        "        self.stage3 = nn.Sequential(\n",
        "            ConvNeXtBlock(dim=384),\n",
        "            ConvNeXtBlock(dim=384),\n",
        "            ConvNeXtBlock(dim=384),\n",
        "            ConvNeXtBlock(dim=384),\n",
        "            ConvNeXtBlock(dim=384),\n",
        "            ConvNeXtBlock(dim=384),\n",
        "            ConvNeXtBlock(dim=384),\n",
        "            ConvNeXtBlock(dim=384),\n",
        "            ConvNeXtBlock(dim=384)\n",
        "        )\n",
        "\n",
        "        # Downsampling 3\n",
        "        self.downsample3 = nn.Sequential(\n",
        "            LayerNorm(384, eps=1e-6, data_format=\"channels_first\"),\n",
        "            nn.Conv2d(384, 768, kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Stage 4\n",
        "        self.stage4 = nn.Sequential(\n",
        "            ConvNeXtBlock(dim=768),\n",
        "            ConvNeXtBlock(dim=768),\n",
        "            ConvNeXtBlock(dim=768)\n",
        "        )\n",
        "\n",
        "        # Final norm and classifier\n",
        "        self.norm = LayerNorm(768, eps=1e-6, data_format=\"channels_first\")\n",
        "        self.fc = nn.Linear(768, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.stage1(x)\n",
        "        x = self.downsample1(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.downsample2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.downsample3(x)\n",
        "        x = self.stage4(x)\n",
        "        x = self.norm(x)\n",
        "        x = x.mean([-2, -1])  # Global average pooling\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class ConvNeXtBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(ConvNeXtBlock, self).__init__()\n",
        "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)\n",
        "        self.norm = LayerNorm(dim, eps=1e-6)\n",
        "        self.pwconv1 = nn.Linear(dim, 4 * dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
        "        self.gamma = nn.Parameter(torch.ones(dim), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.dwconv(x)\n",
        "        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n",
        "        x = self.norm(x)\n",
        "        x = self.pwconv1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pwconv2(x)\n",
        "        x = x * self.gamma.view(1, 1, 1, -1)\n",
        "        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
        "        return x + input\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.eps = eps\n",
        "        self.data_format = data_format\n",
        "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
        "            raise NotImplementedError\n",
        "        self.normalized_shape = (normalized_shape, )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.data_format == \"channels_last\":\n",
        "            return nn.functional.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        elif self.data_format == \"channels_first\":\n",
        "            u = x.mean(1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.eps)\n",
        "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
        "            return x\n",
        "def load_pretrained_weights(model, pretrained_model):\n",
        "    # Load state_dict from pretrained model\n",
        "    pretrained_dict = pretrained_model.state_dict()\n",
        "    model_dict = model.state_dict()\n",
        "\n",
        "    # Filter out unnecessary keys\n",
        "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and 'fc' not in k}\n",
        "\n",
        "    # Overwrite entries in the existing state dict\n",
        "    model_dict.update(pretrained_dict)\n",
        "\n",
        "    # Load the new state dict\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    # Randomly initialize the final fully connected layer\n",
        "    nn.init.normal_(model.fc.weight, std=0.01)\n",
        "    nn.init.zeros_(model.fc.bias)\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Download and prepare CIFAR-100 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # ConvNeXt typically expects 224x224 input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Initialize the model\n",
        "model = ConvNeXtTiny(num_classes=100).to(device)\n",
        "\n",
        "# Download pre-trained weights and assign them\n",
        "pretrained_model = convnext_tiny(weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
        "load_pretrained_weights(model, pretrained_model)\n",
        "\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "exit()\n",
        "\n",
        "# Training loop\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(loader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        if i % 100 == 99:\n",
        "            print(f'[{i+1}] loss: {running_loss/100:.3f} | acc: {100.*correct/total:.2f}%')\n",
        "            running_loss = 0.0\n",
        "    return 100. * correct / total\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    return 100. * correct / total\n",
        "\n",
        "# Train for one epoch\n",
        "print(\"Starting training...\")\n",
        "start_time = time.time()\n",
        "train_accuracy = train_epoch(model, trainloader, criterion, optimizer, device)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n",
        "print(f\"Final training accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_accuracy = evaluate(model, testloader, device)\n",
        "print(f\"Test accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "import os\n",
        "\n",
        "# Create a directory to save checkpoints\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "\n",
        "# Function to save checkpoints\n",
        "def save_checkpoint(model, optimizer, epoch, accuracy, filename):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'accuracy': accuracy,\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "    print(f\"Checkpoint saved: {filename}\")\n",
        "\n",
        "# ... (code for data loading and model initialization remains the same)\n",
        "\n",
        "# Train for a few epochs\n",
        "num_epochs = 5\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    train_accuracy = train_epoch(model, trainloader, criterion, optimizer, device)\n",
        "    end_time = time.time()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n",
        "    print(f\"Training accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_accuracy = evaluate(model, testloader, device)\n",
        "    print(f\"Test accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    checkpoint_filename = f'checkpoints/convnext_epoch_{epoch+1}.pth'\n",
        "    save_checkpoint(model, optimizer, epoch+1, test_accuracy, checkpoint_filename)\n",
        "\n",
        "    print(\"-----------------------------\")\n",
        "\n",
        "print(\"Training finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "zV-AoR9bUmTe",
        "outputId": "59933792-8ea2-4b8c-f42c-42b9660e31fb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_epoch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0acfa66709a9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training completed in {end_time - start_time:.2f} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_epoch' is not defined"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "import time\n",
        "for i in range(10):\n",
        "  start_time = time.time()\n",
        "  train_accuracy = train_epoch(model, trainloader, criterion, optimizer, device)\n",
        "  end_time = time.time()\n",
        "  print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n",
        "  print(f\"Final training accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "  # Evaluate on test set\n",
        "  test_accuracy = evaluate(model, testloader, device)\n",
        "  print(f\"Test accuracy: {test_accuracy:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNoV66UN0iwGU2JI8+NBJ8l",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}